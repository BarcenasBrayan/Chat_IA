import customtkinter as ctk
from transformers import BertTokenizer, BertForSequenceClassification, GPT2LMHeadModel, GPT2Tokenizer
import torch
import re

chat_history = ""  # Historial acumulado

def procesar_respuesta(respuesta, prompt, max_chars=800):
    texto = respuesta[len(prompt):].strip()
    texto = re.sub(r'(Usuario:|<\|endoftext\|>)', '', texto, flags=re.IGNORECASE).strip()

    # Cortar si empieza a escribir el siguiente turno del usuario
    if "Usuario:" in texto:
        texto = texto.split("Usuario:")[0].strip()
    if "Enrique:" in texto[1:]:  # Si genera "Enrique:" nuevamente, tambi茅n cortamos ah铆
        texto = texto.split("Enrique:")[0].strip()

    if len(texto) > max_chars:
        texto = texto[:max_chars].rsplit(" ", 1)[0] + "..."

    return texto


# Cargar modelos de Hugging Face
# Aseg煤rate de que los directorios './beto_finetuned' y './gpt2-finetuned'
# contengan los modelos y tokenizadores pre-entrenados y/o fine-tuned.
try:
    beto_model = BertForSequenceClassification.from_pretrained("./beto_finetuned")
    beto_tokenizer = BertTokenizer.from_pretrained("./beto_finetuned")
    beto_model.eval() # Pone el modelo en modo de evaluaci贸n (desactiva dropout, etc.)

    gpt2_model = GPT2LMHeadModel.from_pretrained("./gpt2-finetuned")
    gpt2_tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-finetuned")
    gpt2_model.eval() # Pone el modelo en modo de evaluaci贸n
    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token # Configura el token de padding para GPT-2
except Exception as e:
    print(f"Error al cargar los modelos: {e}")
    print("Aseg煤rate de que los modelos 'beto_finetuned' y 'gpt2-finetuned' est茅n en el directorio correcto.")
    # Puedes a帽adir un sys.exit() aqu铆 si los modelos son esenciales para la aplicaci贸n
    # import sys
    # sys.exit(1)


# Mapeo de IDs de predicci贸n a etiquetas de intenci贸n
id2label = {
    0: "info_clave_texto",
    1: "recomendacion_peliculas",
    2: "sobre_bot",
    3: "saludo"
}

# Plantillas de prompt para cada intenci贸n
plantillas = {
    "info_clave_texto": "Usuario: {entrada}\nResumen:",
    "recomendacion_peliculas": "Usuario: {entrada}\nEnrique:",
    "sobre_bot": "Usuario: {entrada}\nEnrique:",
    "saludo": "Usuario: {entrada}\nEnrique:"
}

def responder():
    global chat_history

    entrada = input_entry.get().strip()
    if not entrada:
        return

    # Clasificaci贸n de intenci贸n con BETO
    tokens = beto_tokenizer(entrada, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        salida = beto_model(**tokens)
        pred = torch.argmax(salida.logits, dim=1).item()
        intencion = id2label[pred]

    # A帽adir turno del usuario al historial
    chat_history += f"Usuario: {entrada}\nEnrique:"

    # Recortar historial si es muy largo
    max_tokens = 700  # Por seguridad, mantenerlo debajo de 1024 para GPT-2 small
    tokens_hist = gpt2_tokenizer.encode(chat_history)
    if len(tokens_hist) > max_tokens:
        tokens_hist = tokens_hist[-max_tokens:]
        chat_history = gpt2_tokenizer.decode(tokens_hist)

    # Crear entrada para GPT-2
    gpt_input = gpt2_tokenizer(chat_history, return_tensors="pt")
    with torch.no_grad():
        output = gpt2_model.generate(
            **gpt_input,
            max_length=len(gpt_input["input_ids"][0]) + 500,
            do_sample=True,
            top_p=0.90,
            temperature=0.5,
            pad_token_id=gpt2_tokenizer.eos_token_id
        )

    respuesta = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)
    respuesta_final = procesar_respuesta(respuesta, chat_history)

    # A帽adir respuesta del bot al historial
    chat_history += f" {respuesta_final}\n"

    # Mostrar en la interfaz
    output_box.configure(state="normal")
    output_box.insert("end", f" T煤: {entrada}\n")
    output_box.insert("end", f" Enrique: {respuesta_final}\n\n")
    output_box.configure(state="disabled")
    input_entry.delete(0, "end")


# Configuraci贸n de la ventana principal de CustomTkinter
ctk.set_appearance_mode("light") # Establece el modo de apariencia a "light"
ctk.set_default_color_theme("blue") # Establece el tema de color por defecto a "blue" (un tema predefinido v谩lido)

app = ctk.CTk() # Crea la ventana principal de la aplicaci贸n
app.geometry("700x600") # Establece el tama帽o inicial de la ventana
app.title("Enrique - IA Asistente") # Establece el t铆tulo de la ventana
app.configure(fg_color="orange") # Fondo de la ventana principal naranja

# Crear y configurar la caja de texto para la salida (conversaci贸n)
output_box = ctk.CTkTextbox(app, width=650, height=400, font=("Arial", 12), fg_color="salmon") # Fondo salm贸n
output_box.grid(row=0, column=0, padx=20, pady=20, sticky="nsew") # Posiciona y permite que se expanda
output_box.configure(state="disabled") # La caja de texto comienza deshabilitada

# Crear y configurar el campo de entrada para el usuario
input_entry = ctk.CTkEntry(app, width=500, placeholder_text="Escribe tu mensaje aqu铆...", text_color="black") # Texto del usuario negro
input_entry.grid(row=1, column=0, padx=20, pady=(0, 10), sticky="w") # Posiciona a la izquierda

# Crear y configurar el bot贸n de enviar
send_button = ctk.CTkButton(app, text="Enviar", command=responder, fg_color="black") # Bot贸n negro
send_button.grid(row=1, column=0, padx=20, pady=(0, 10), sticky="e") # Posiciona a la derecha

# Configurar el grid para que la caja de texto se expanda verticalmente
app.grid_rowconfigure(0, weight=1)
app.grid_columnconfigure(0, weight=1)

# Iniciar el bucle principal de la aplicaci贸n
app.mainloop()