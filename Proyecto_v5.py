import customtkinter as ctk
from transformers import BertTokenizer, BertForSequenceClassification, GPT2LMHeadModel, GPT2Tokenizer
import torch
import re
import pandas as pd
import random
import numpy as np
import os # Para manejo de archivos/rutas

# --- 1. CONFIGURACI√ìN E INICIALIZACI√ìN RAG ---

# Cargar la base de datos de pel√≠culas
try:
    df_peliculas = pd.read_csv("C:\\Users\\Brayan\\Desktop\\Python\\DatasetPruebas\\peliculas_populares_tmdb.csv")
    
    # 1. Manejar NaNs (nulos) y limpiar datos
    df_peliculas = df_peliculas.fillna({'A√±o': np.nan, 'G√©nero': '', 'Rese√±a': 'Sinopsis no disponible.', 'Recomendaciones': ''})
    
    # Asegurar que 'A√±o' se maneje como entero, reemplazando NaNs con '????' para el display
    df_peliculas['A√±o_Display'] = df_peliculas['A√±o'].apply(
        lambda x: str(int(x)) if pd.notna(x) and x > 1900 else '????'
    )
    
    # Creamos un campo de "Puntuaci√≥n/Popularidad" simulado si no existe
    if 'Puntuacion_Calidad' not in df_peliculas.columns:
         df_peliculas['Puntuacion_Calidad'] = np.random.rand(len(df_peliculas)) * 5 + 5
    
    # Limpiamos los g√©neros para la b√∫squeda (se convierten a min√∫sculas)
    df_peliculas['G√©nero_Lista'] = df_peliculas['G√©nero'].apply(
        lambda x: [g.strip().lower() for g in str(x).split(',')]
    )
    print("Base de datos de pel√≠culas cargada y limpia correctamente.")
except FileNotFoundError:
    print("ADVERTENCIA: Archivo 'peliculas_populares_tmdb.csv' no encontrado.")
    df_peliculas = pd.DataFrame()
except Exception as e:
    print(f"Error al cargar la base de datos de pel√≠culas: {e}")
    df_peliculas = pd.DataFrame()


chat_history = ""  # Historial acumulado

def procesar_respuesta(respuesta, prompt, max_chars=800):
    """
    Funci√≥n para limpiar la respuesta generada por GPT-2 de artefactos 
    (solo para intenciones que NO son recomendaci√≥n de pel√≠culas).
    """
    # ... (El c√≥digo de limpieza se mantiene igual) ...
    texto = respuesta[len(prompt):].strip()
    # Limpiar el marcador de contexto inyectado si se filtr√≥ por error
    texto = re.sub(r'Datos_Pel√≠cula:.*', '', texto, flags=re.IGNORECASE).strip() 
    texto = re.sub(r'(Usuario:|<\|endoftext\|>)', '', texto, flags=re.IGNORECASE).strip()

    if "Usuario:" in texto:
        texto = texto.split("Usuario:")[0].strip()
    if "Enrique:" in texto[1:]:
        texto = texto.split("Enrique:")[0].strip()

    if len(texto) > max_chars:
        texto = texto[:max_chars].rsplit(" ", 1)[0] + "..."

    return texto


# Cargar modelos de Hugging Face
try:
    beto_model = BertForSequenceClassification.from_pretrained("./beto_finetuned")
    beto_tokenizer = BertTokenizer.from_pretrained("./beto_finetuned")
    beto_model.eval()

    gpt2_model = GPT2LMHeadModel.from_pretrained("./gpt2-finetuned")
    gpt2_tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-finetuned")
    gpt2_model.eval()
    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token
except Exception as e:
    print(f"Error al cargar los modelos: {e}")
    print("Aseg√∫rate de que los modelos 'beto_finetuned' y 'gpt2-finetuned' est√©n en el directorio correcto.")


# Mapeo de IDs de predicci√≥n a etiquetas de intenci√≥n
id2label = {
    0: "info_clave_texto",
    1: "recomendacion_peliculas",
    2: "sobre_bot",
    3: "saludo"
}

# --- 2. L√ìGICA DE RECUPERACI√ìN (RAG) SUTIL ---

def obtener_prompt_enriquecido(entrada, chat_history):
    """
    Recupera la informaci√≥n del CSV, crea un prompt enriquecido para GPT-2 
    y retorna los datos factuales de la pel√≠cula.
    """
    global df_peliculas
    
    if df_peliculas.empty:
        return None, None 

    entrada_lower = entrada.lower()
    genero_buscado = None
    generos_disponibles = set(g for sublist in df_peliculas['G√©nero_Lista'] for g in sublist)
    
    # Buscar el g√©nero en la entrada
    for gen in generos_disponibles:
        if gen in entrada_lower:
            genero_buscado = gen
            break

    candidatas = df_peliculas

    if genero_buscado:
        candidatas = df_peliculas[df_peliculas['G√©nero_Lista'].apply(lambda x: genero_buscado in x)]
    
    if candidatas.empty:
        # Fallback a las pel√≠culas m√°s populares si no hay coincidencias de g√©nero o no se encontr√≥ el g√©nero.
        top_n = max(5, int(len(df_peliculas) * 0.3))
        candidatas = df_peliculas.sort_values(by='Puntuacion_Calidad', ascending=False).head(top_n)

    # 1. Seleccionamos la pel√≠cula
    if candidatas.empty:
        return None, None # No hay pel√≠culas disponibles
        
    pelicula_seleccionada = candidatas.sample(1).iloc[0]
    
    # 2. Recuperaci√≥n y limpieza de datos Factuales
    titulo = pelicula_seleccionada.get('Pel√≠cula', 'T√≠tulo Desconocido').replace('"', '').strip()
    anio = pelicula_seleccionada.get('A√±o_Display', '????').strip()
    genero = pelicula_seleccionada.get('G√©nero', 'Sin G√©nero').strip()
    
    rese√±a_raw = pelicula_seleccionada.get('Rese√±a', 'Sinopsis no disponible.').strip()
    # Limpieza estricta de la sinopsis
    rese√±a_limpia = re.sub(r'[\r\n]+', ' ', rese√±a_raw).strip()
    rese√±a_limpia = re.sub(r' +', ' ', rese√±a_limpia)

    # 3. CONSTRUCCI√ìN DEL PROMPT ENRIQUECIDO
    # Inyectamos los datos clave de manera estructurada en el prompt de GPT-2
    contexto_factual = (
        f"Datos_Pel√≠cula: T√≠tulo=\"{titulo}\" A√±o={anio} G√©neros=\"{genero}\" Sinopsis=\"{rese√±a_limpia}\""
    )
    
    # Concatenamos el historial, la entrada del usuario y el contexto factual
    # La clave es terminar con "Enrique: " para que GPT-2 genere la introducci√≥n.
    prompt_gpt2 = (
        f"{chat_history}"
        f"Usuario: {entrada}\n"
        f"{contexto_factual}\n"
        f"Enrique: "
    )
    
    # Informaci√≥n Factual Clara para la inyecci√≥n final
    info_recuperada = {
        "titulo": titulo,
        "anio": anio,
        "genero": genero,
        "sinopsis": rese√±a_limpia
    }
    
    return prompt_gpt2, info_recuperada


# --- 3. FUNCI√ìN DE RESPUESTA CON L√ìGICA RAG INTEGRADA ---

def responder():
    global chat_history

    entrada = input_entry.get().strip()
    if not entrada:
        return

    # Clasificaci√≥n de intenci√≥n con BETO
    tokens = beto_tokenizer(entrada, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        salida = beto_model(**tokens)
        pred = torch.argmax(salida.logits, dim=1).item()
        intencion = id2label[pred]
    
    respuesta_final = ""
    
    # --------------------------- L√ìGICA RAG SUTIL ---------------------------
    if intencion == "recomendacion_peliculas":
        
        # Recortar historial si es muy largo (mismo c√≥digo de antes)
        max_tokens = 700 
        tokens_hist = gpt2_tokenizer.encode(chat_history)
        if len(tokens_hist) > max_tokens:
            tokens_hist = tokens_hist[-max_tokens:]
            chat_history = gpt2_tokenizer.decode(tokens_hist)
            
        # 1. Recuperar Prompt y Datos Factuales
        prompt_gpt2, info_recuperada = obtener_prompt_enriquecido(entrada, chat_history)

        if prompt_gpt2 is None:
             respuesta_final = "Lo siento, mi base de datos de pel√≠culas no est√° disponible o no hay coincidencias."
        else:
            # 2. GENERACI√ìN con el PROMPT ENRIQUECIDO
            gpt_input = gpt2_tokenizer(prompt_gpt2, return_tensors="pt")
            
            with torch.no_grad():
                output = gpt2_model.generate(
                    **gpt_input,
                    temperature=0.6, # Reducir temperatura para m√°s coherencia
                    top_p=0.9,
                    max_length=len(gpt_input["input_ids"][0]) + 40, # Generar SOLO la introducci√≥n/cierre
                    do_sample=True,
                    pad_token_id=gpt2_tokenizer.eos_token_id
                )

            respuesta_gpt2_cruda = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)
            texto_generado = respuesta_gpt2_cruda[len(prompt_gpt2):].strip()
            
            # 3. RECONSTRUCCI√ìN FINAL F√ÅCTUAL Y NATURAL
            
            # Usamos la primera parte generada por GPT-2 como introducci√≥n
            introduccion = texto_generado.split('\n')[0].split('.')[0] 
            
            # Si GPT-2 no gener√≥ una buena introducci√≥n, usamos una por defecto
            if len(introduccion) < 10 or 'pelicula' not in introduccion.lower(): 
                introduccion = "Aqu√≠ tienes una buena para empezar:"
            else:
                 introduccion += "." # Aseguramos el punto final de la frase

            # Inyecci√≥n de los datos REALES (el contenido no alucinado)
            respuesta_factual_y_natural = (
                f"{introduccion.strip()}\n"
                f"\"**{info_recuperada['titulo']}**\" ({info_recuperada['anio']})\n"
                f"G√©nero: {info_recuperada['genero']}\n"
                f"{info_recuperada['sinopsis']}"
            )
            
            respuesta_final = respuesta_factual_y_natural

            # A√±adir turno del usuario y la respuesta FINAL al historial
            chat_history += f"Usuario: {entrada}\nEnrique: {respuesta_final}\n"

    # --------------------------- L√ìGICA GENERATIVA (No RAG) ---------------------------
    else:
        # A√±adir turno del usuario al historial
        chat_history += f"Usuario: {entrada}\nEnrique:"

        # Recortar historial
        max_tokens = 700 
        tokens_hist = gpt2_tokenizer.encode(chat_history)
        if len(tokens_hist) > max_tokens:
            tokens_hist = tokens_hist[-max_tokens:]
            chat_history = gpt2_tokenizer.decode(tokens_hist)

        # Crear entrada para GPT-2 y generar (temperatura m√°s alta para creatividad)
        gpt_input = gpt2_tokenizer(chat_history, return_tensors="pt")
        with torch.no_grad():
            output = gpt2_model.generate(
                **gpt_input,
                max_length=len(gpt_input["input_ids"][0]) + 500,
                do_sample=True,
                top_p=0.95,
                temperature=0.7,
                pad_token_id=gpt2_tokenizer.eos_token_id
            )

        respuesta = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)
        respuesta_final = procesar_respuesta(respuesta, chat_history)

        # A√±adir respuesta del bot al historial
        chat_history += f" {respuesta_final}\n"

    # --- Mostrar en la interfaz ---
    output_box.configure(state="normal")
    output_box.insert("end", f"üßë T√∫: {entrada}\n")
    output_box.insert("end", f"ü§ñ Enrique: {respuesta_final}\n\n")
    output_box.configure(state="disabled")
    input_entry.delete(0, "end")


# --- INTERFAZ GR√ÅFICA (sin cambios) ---
ctk.set_appearance_mode("light") 
ctk.set_default_color_theme("blue") 

app = ctk.CTk()
app.geometry("700x600")
app.title("Enrique - IA Asistente (RAG Sutil)")
app.configure(fg_color="orange") 

output_box = ctk.CTkTextbox(app, width=650, height=400, font=("Arial", 12), fg_color="salmon")
output_box.grid(row=0, column=0, padx=20, pady=20, sticky="nsew")
output_box.configure(state="disabled")

input_entry = ctk.CTkEntry(app, width=500, placeholder_text="Escribe tu mensaje aqu√≠...", text_color="black")
input_entry.grid(row=1, column=0, padx=20, pady=(0, 10), sticky="w")

send_button = ctk.CTkButton(app, text="Enviar", command=responder, fg_color="black")
send_button.grid(row=1, column=0, padx=20, pady=(0, 10), sticky="e")

app.grid_rowconfigure(0, weight=1)
app.grid_columnconfigure(0, weight=1)

app.mainloop()